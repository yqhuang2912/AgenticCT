#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
CT Image Quality Evaluation (QE) Tool Trainer
---------------------------------------------
Train a small CNN to predict degradation severities for:
  - LDCT: none/low/medium/high
  - LACT: none/low/medium/high
  - SVCT: none/low/medium/high

Key features (aligned with your LACT trainer style):
  - Trainer class with train_step / validation_step / train loop
  - SwanLab logging: metrics + images every N steps, and val first batch
  - Best checkpoint saving by val_loss and val_acc_avg
  - Optional dataset_info preprocessing:
        merge multiple raw dataset_info.csv -> qe_dataset_info.csv
        include clean samples from single-degradation label_paths
        degradations column is regenerated from the 3 severities (Method B)

NOTE on normalization:
  - This script does NOT do HU conversion or extra normalization.
  - It assumes your stored .npy is already in a stable numeric domain.
  - We only handle shape normalization to (1,H,W) and optional clipping via --clip_input.

Author: Generated by ChatGPT
"""

from __future__ import annotations

import os
import ast
import logging
import argparse
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from torchvision.models import resnet18

from utils.model_save_load import save_model
from pretrain.supervisor.models.ctqe_model import CTQEModel
from pretrain.supervisor.dataset import CTQEDataset, LABELS, LABEL2ID, ID2LABEL

from utils.swanlab_utils import (
    config_swanlab,
    log_metrics_to_swanlab,
)

logger = logging.getLogger(__name__)


# ---------------------------
# Utilities
# ---------------------------
def seed_everything(seed: int) -> None:
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


# ---------------------------
# Dataset
# ---------------------------
# Moved to pretrain.supervisor.dataset.CTQEDataset


# ---------------------------
# Model
# ---------------------------
# Moved to pretrain.supervisor.models.ctqe_model



# ---------------------------
# Trainer (LACT-style)
# ---------------------------
@dataclass
class TrainConfig:
    dataset_info_file: str
    epochs: int
    lr: float
    train_ratio: float
    batch_size: int
    num_workers: int
    vis_every_n_step: int

    save_dir_root: str
    run_name: str
    project_name: str

    pretrained: bool
    label_smoothing: float

    # preprocessing options
    preprocess: bool = False
    raw_dataset_info_files: Optional[List[str]] = None
    qe_dataset_info_out: Optional[str] = None
    dedup_clean: bool = False
    max_clean: Optional[int] = None


class Trainer:
    def __init__(self, cfg: TrainConfig):
        self.cfg = cfg
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.model = CTQEModel(pretrained=cfg.pretrained).to(self.device)
        self.model_config = {
            "labels": LABELS,
            "label2id": LABEL2ID,
            "id2label": ID2LABEL,
            "arch": "resnet18_1ch",
        }

        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=cfg.lr, weight_decay=1e-4)
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=cfg.epochs)

        self.epochs = cfg.epochs
        self.save_dir = os.path.join(cfg.save_dir_root, cfg.run_name)
        os.makedirs(self.save_dir, exist_ok=True)

        self.vis_every_n_step = cfg.vis_every_n_step
        self.global_step = 0

    @staticmethod
    def _acc(logits: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        pred = logits.argmax(dim=1)
        return (pred == y).float().mean()

    def train_step(self, batch, batch_idx: int):
        x, y = batch
        x = x.to(self.device, non_blocking=True)
        y = {k: v.to(self.device, non_blocking=True) for k, v in y.items()}

        self.optimizer.zero_grad(set_to_none=True)
        out = self.model(x)

        loss_ldct = F.cross_entropy(out["ldct"], y["ldct"], label_smoothing=self.cfg.label_smoothing)
        loss_lact = F.cross_entropy(out["lact"], y["lact"], label_smoothing=self.cfg.label_smoothing)
        loss_svct = F.cross_entropy(out["svct"], y["svct"], label_smoothing=self.cfg.label_smoothing)
        loss = (loss_ldct + loss_lact + loss_svct) / 3.0

        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        self.optimizer.step()

        with torch.no_grad():
            acc_ldct = self._acc(out["ldct"], y["ldct"])
            acc_lact = self._acc(out["lact"], y["lact"])
            acc_svct = self._acc(out["svct"], y["svct"])
            acc_avg = (acc_ldct + acc_lact + acc_svct) / 3.0

        metrics = {
            "loss": float(loss.item()),
            "acc_ldct": float(acc_ldct.item()),
            "acc_lact": float(acc_lact.item()),
            "acc_svct": float(acc_svct.item()),
            "acc_avg": float(acc_avg.item()),
        }

        # logger.info(
        #     f"[Train] step={self.global_step} "
        #     f"loss={metrics['loss']:.4f}, "
        #     f"acc_avg={metrics['acc_avg']:.4f}, "
        #     f"ldct={metrics['acc_ldct']:.4f}, "
        #     f"lact={metrics['acc_lact']:.4f}, "
        #     f"svct={metrics['acc_svct']:.4f}"
        # )
        

        if self.vis_every_n_step and (batch_idx % self.vis_every_n_step == 0):
            log_metrics_to_swanlab(metrics, mode="train", step=self.global_step)

        self.global_step += 1
        return metrics

    @torch.no_grad()
    def validation_step(self, batch, batch_idx: int = 0):
        x, y = batch
        x = x.to(self.device, non_blocking=True)
        y = {k: v.to(self.device, non_blocking=True) for k, v in y.items()}
        out = self.model(x)

        loss_ldct = F.cross_entropy(out["ldct"], y["ldct"])
        loss_lact = F.cross_entropy(out["lact"], y["lact"])
        loss_svct = F.cross_entropy(out["svct"], y["svct"])
        loss = (loss_ldct + loss_lact + loss_svct) / 3.0

        acc_ldct = self._acc(out["ldct"], y["ldct"])
        acc_lact = self._acc(out["lact"], y["lact"])
        acc_svct = self._acc(out["svct"], y["svct"])
        acc_avg = (acc_ldct + acc_lact + acc_svct) / 3.0

        metrics = {
            "val_loss": float(loss.item()),
            "val_acc_ldct": float(acc_ldct.item()),
            "val_acc_lact": float(acc_lact.item()),
            "val_acc_svct": float(acc_svct.item()),
            "val_acc_avg": float(acc_avg.item()),
        }

        # logger.info(
        #     f"[Val] step={self.global_step} "
        #     f"loss={metrics['val_loss']:.4f}, "
        #     f"acc_avg={metrics['val_acc_avg']:.4f}, "
        #     f"ldct={metrics['val_acc_ldct']:.4f}, "
        #     f"lact={metrics['val_acc_lact']:.4f}, "
        #     f"svct={metrics['val_acc_svct']:.4f}"
        # )
        # print(
        #     f"[Val] step={self.global_step} "
        #     f"loss={metrics['val_loss']:.4f}, "
        #     f"acc_avg={metrics['val_acc_avg']:.4f}, "
        #     f"ldct={metrics['val_acc_ldct']:.4f}, "
        #     f"lact={metrics['val_acc_lact']:.4f}, "
        #     f"svct={metrics['val_acc_svct']:.4f}"
        # )

        log_metrics_to_swanlab(metrics, mode="val", step=self.global_step)

        return metrics

    def train(self, train_loader: DataLoader, val_loader: DataLoader):
        best_val_loss = float("inf")
        best_val_acc = float("-inf")

        for epoch in range(self.epochs):
            logger.info(f"========== Epoch [{epoch+1}/{self.epochs}] ==========")
            self.model.train()
            for batch_idx, batch in enumerate(train_loader):
                metrics = self.train_step(batch, batch_idx)
                if (batch_idx + 1) % self.vis_every_n_step == 0:
                    print(
                        f"[Train: {epoch+1}/{self.epochs} {batch_idx+1}/{len(train_loader)}] "
                        f"loss={metrics['loss']:.4f}, "
                        f"acc_avg={metrics['acc_avg']:.4f}, "
                        f"ldct={metrics['acc_ldct']:.4f}, "
                        f"lact={metrics['acc_lact']:.4f}, "
                        f"svct={metrics['acc_svct']:.4f}"
                    )

            self.scheduler.step()

            # validation
            self.model.eval()
            val_losses, val_accs = [], []
            val_ldct, val_lact, val_svct = [], [], []

            for batch_idx, batch in enumerate(val_loader):
                metrics = self.validation_step(batch, batch_idx)
                val_losses.append(metrics["val_loss"])
                val_accs.append(metrics["val_acc_avg"])
                val_ldct.append(metrics["val_acc_ldct"])
                val_lact.append(metrics["val_acc_lact"])
                val_svct.append(metrics["val_acc_svct"])

            avg_val_loss = float(np.mean(val_losses))
            avg_val_acc = float(np.mean(val_accs))
            avg_ldct = float(np.mean(val_ldct))
            avg_lact = float(np.mean(val_lact))
            avg_svct = float(np.mean(val_svct))

            print(
                f"[Epoch {epoch+1}/{self.epochs}] "
                f"Val Loss={avg_val_loss:.4f}, "
                f"AccAvg={avg_val_acc:.4f}, "
                f"LDCT={avg_ldct:.4f}, LACT={avg_lact:.4f}, SVCT={avg_svct:.4f}"
            )

            log_metrics_to_swanlab(
                {
                    "epoch_val_loss": avg_val_loss,
                    "epoch_val_acc_avg": avg_val_acc,
                    "epoch_val_acc_ldct": avg_ldct,
                    "epoch_val_acc_lact": avg_lact,
                    "epoch_val_acc_svct": avg_svct,
                },
                mode="val_epoch",
                step=epoch,
            )

            # save best models
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                save_model(
                    self.model,
                    self.model_config,
                    os.path.join(self.save_dir, "best_val_loss_model.pth"),
                )
                logger.info(
                    f"Best val loss model saved at epoch {epoch+1}, loss={best_val_loss:.4f}"
                )

            if avg_val_acc > best_val_acc:
                best_val_acc = avg_val_acc
                save_model(
                    self.model,
                    self.model_config,
                    os.path.join(self.save_dir, "best_val_acc_model.pth"),
                )
                logger.info(
                    f"Best val acc model saved at epoch {epoch+1}, acc={best_val_acc:.4f}"
                )

        logger.info("Training complete.")


# ---------------------------
# Train entry
# ---------------------------
def make_loaders(cfg: TrainConfig) -> Tuple[DataLoader, DataLoader]:
    train_dataset = CTQEDataset(
        dataset_info_file=cfg.dataset_info_file,
        split="train",
        train_ratio=cfg.train_ratio,
        seed=cfg.seed,
    )
    val_dataset = CTQEDataset(
        dataset_info_file=cfg.dataset_info_file,
        split="test",
        train_ratio=cfg.train_ratio,
        seed=cfg.seed,
    )

    dl_kwargs = dict(
        batch_size=cfg.batch_size,
        pin_memory=True,
        num_workers=cfg.num_workers,
        persistent_workers=(cfg.num_workers > 0),
    )
    if cfg.num_workers > 0:
        dl_kwargs["prefetch_factor"] = 4

    train_loader = DataLoader(train_dataset, shuffle=True, **dl_kwargs)
    val_loader = DataLoader(val_dataset, shuffle=False, **dl_kwargs)
    return train_loader, val_loader


def parse_args() -> TrainConfig:
    parser = argparse.ArgumentParser(description="CT QE Training with SwanLab (LACT-style)")

    # dataset
    parser.add_argument("--dataset_info_file", type=str, default="/data/hyq/codes/AgenticCT/data/qe/deeplesion/dataset_info_qe.csv",
                        help="QE dataset_info.csv for training")

    # training
    parser.add_argument("--epochs", type=int, default=40)
    parser.add_argument("--lr", type=float, default=1e-4)
    parser.add_argument("--train_ratio", type=float, default=0.75)
    parser.add_argument("--batch_size", type=int, default=128)
    parser.add_argument("--num_workers", type=int, default=8)
    parser.add_argument("--vis_every_n_step", type=int, default=100)

    # model
    parser.add_argument("--pretrained", action="store_true", help="Use pretrained backbone if available")
    parser.add_argument("--label_smoothing", type=float, default=0.05)

    # io/logging/swanlab
    parser.add_argument("--save_dir_root", type=str, default="/data/hyq/codes/AgenticCT/src/pretrain/outputs/deeplesion/ctqe")
    parser.add_argument("--project_name", type=str, default="deeplesion_ctqe")
    parser.add_argument("--run_name", type=str, default="default")

    # misc
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--swanlab_mode", type=str, default="cloud", choices=["cloud", "local"])

    args = parser.parse_args()

    # resolve dataset_info_file
    if not args.dataset_info_file:
        raise ValueError("Please provide --dataset_info_file")
    
    cfg = TrainConfig(
        dataset_info_file=args.dataset_info_file,
        epochs=args.epochs,
        lr=args.lr,
        train_ratio=args.train_ratio,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        vis_every_n_step=args.vis_every_n_step,
        save_dir_root=args.save_dir_root,
        run_name=args.run_name,
        project_name=args.project_name,
        pretrained=args.pretrained,
        label_smoothing=args.label_smoothing,
    )
    cfg.seed = args.seed  # attach to config for dataset split
    cfg.swanlab_mode = args.swanlab_mode
    return cfg


def main():
    cfg = parse_args()
    seed_everything(cfg.seed)

    # prepare save/log dirs
    save_dir = os.path.join(cfg.save_dir_root, cfg.run_name)
    os.makedirs(save_dir, exist_ok=True)

    logging.basicConfig(
        filename=os.path.join(cfg.save_dir_root, f"{cfg.run_name}.log"),
        level=logging.DEBUG,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )
    logger.info("Starting CT QE training with configuration:")
    logger.info(cfg)

    # swanlab init
    config_swanlab(
        project_name=cfg.project_name,
        run_name=cfg.run_name,
        config=cfg.__dict__,
        logdir=os.path.join(cfg.save_dir_root, "swanlab_logs"),
        mode=cfg.swanlab_mode,  # "cloud" or "local"
        description=f"CT QE tool (3-head severity), run={cfg.run_name}",
    )

    train_loader, val_loader = make_loaders(cfg)
    trainer = Trainer(cfg)
    trainer.train(train_loader, val_loader)


if __name__ == "__main__":
    main()
